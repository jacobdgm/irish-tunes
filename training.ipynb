{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers tokenizers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the user's convenience `tokenizers` provides some very high-level classes encapsulating\n",
    "# the overall pipeline for various well-known tokenization algorithm. \n",
    "# Everything described below can be replaced by the ByteLevelBPETokenizer class. \n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Lowercase, NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\n",
    "# First we create an empty Byte-Pair Encoding model (i.e. not trained model)\n",
    "_tokenizer = Tokenizer(BPE())\n",
    "\n",
    "# Our tokenizer also needs a pre-tokenizer responsible for converting the input to a ByteLevel representation.\n",
    "_tokenizer.pre_tokenizer = ByteLevel()\n",
    "\n",
    "# And finally, let's plug a decoder so we can recover from a tokenized input to the original one\n",
    "_tokenizer.decoder = ByteLevelDecoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    special_tokens=[\n",
    "        (\"<s>\", 0),\n",
    "        (\"</s>\", 1),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# We initialize our trainer, giving him the details about the vocabulary we want to generate\n",
    "trainer = BpeTrainer(vocab_size=500, show_progress=True, initial_alphabet=ByteLevel.alphabet(), special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"</s>\",\n",
    "            \"<pad>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\"\n",
    "        ])\n",
    "\n",
    "\n",
    "_tokenizer.train(files=[\"2021_07_20_tokenized_tunes.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the tokenized data in our specified folder \n",
    "import os\n",
    "save_path = 'tokenized_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, GenerateMode\n",
    "\n",
    "dataset = load_dataset('text', data_files='2021_07_20_tokenized_tunes.txt', download_mode = GenerateMode.FORCE_REDOWNLOAD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dataset['train'].train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = datasets.map(lambda x: tokenizer(x['text'], truncation=True, max_length=512), num_proc=4, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Dataset Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = 'line'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = 'window'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "def preprocess_window(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "def preprocess_line(examples):\n",
    "    results = {\n",
    "        k: v[:max_length] for k,v in examples.items()\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if strategy == 'line': \n",
    "    preprocessed_data = tokenized_dataset.map(\n",
    "        preprocess_line,\n",
    "        num_proc=4,\n",
    "        batched=True,\n",
    "        batch_size=100\n",
    "    )\n",
    "elif strategy == 'window':\n",
    "    preprocessed_data = tokenized_dataset.map(\n",
    "        preprocess_window,\n",
    "        num_proc=4,\n",
    "        batched=True,\n",
    "        batch_size=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "class DataCollatorWithLabelPadding(DataCollatorWithPadding):\n",
    "    def __call__(self, features):\n",
    "        batch = super().__call__(features)\n",
    "        batch['labels'] = batch['input_ids'].detach().clone()\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if strategy == 'line':\n",
    "    data_collator = DataCollatorWithLabelPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding='longest',\n",
    "        max_length=512,\n",
    "        pad_to_multiple_of=4,\n",
    "    )\n",
    "elif strategy == 'window':\n",
    "     data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding='longest',\n",
    "        max_length=512,\n",
    "        pad_to_multiple_of=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I chose the hyperparameters here to get an architecture that\n",
    "# had roughly the same number of paramters as FolkRNN. But I changed a lot of \n",
    "# things and IDK what the trade offs are.\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Config\n",
    "config = GPT2Config(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    n_positions = 512, \n",
    "    n_embd = 384,\n",
    "    n_ctx = 512,\n",
    "    n_layer = 6,\n",
    "    n_inner = 6,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    unk_token_id=tokenizer.unk_token_id,\n",
    "    mask_token_id=tokenizer.mask_token_id,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./results',          # output directory\n",
    "  num_train_epochs=10,              # total # of training epochs\n",
    "  per_device_train_batch_size=16,  # batch size per device during training\n",
    "  per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "  evaluation_strategy = \"epoch\",\n",
    "   # directory for storing logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=preprocessed_data['train'],\n",
    "    eval_dataset=preprocessed_data['test'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(start_text = \"a b\"):\n",
    "    # encoding the input text\n",
    "    input_ids = tokenizer.encode(start_text, return_tensors='pt')\n",
    "    # getting out output\n",
    "    beam_output = model.generate(\n",
    "      input_ids,\n",
    "      do_sample=True,\n",
    "      max_length = 128,\n",
    "      top_k=50,\n",
    "      temperature = 0.8,\n",
    "      num_return_sequences=5,\n",
    "      beams=5, \n",
    "    \n",
    "    )\n",
    "    return beam_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate()\n",
    "for i in range(0, 5):\n",
    "    print(tokenizer.decode(output[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO / things to try:\n",
    "#### Pre-processing:\n",
    "    - possibly shift to the same scale \n",
    "#### Tokenization:\n",
    "    - look at other archtitectures for domain specific special tokens\n",
    "    - if we can't get good results, data process + train manually to use our own function \n",
    "#### Data-processing:\n",
    "    - find a better way to create training labels\n",
    "    - tune batch_size and block_size\n",
    "#### Model\n",
    "    - tune architecture parameters\n",
    "    - try alternative architectures (ProphetNet?)\n",
    "#### Training\n",
    "    - tune training parameters\n",
    "#### Generation\n",
    "    - figure out what all of the parameters in generate do \n",
    "    - write our own generation code?\n",
    "#### Evaluation\n",
    "    - look into quality metrics from existing research\n",
    "    - write some code to detect training data plagiarism (generation spitting back an input)\n",
    "    - write some code to check for structural integrity ?\n",
    "#### Misc\n",
    "    - figure out a good workflow to train and share model versions \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('checkpoint')\n",
    "model = AutoModelForCausalLM.from_pretrained('checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### retrain the model with \n",
    "### add the metadata\n",
    "\n",
    "## figure out how to best generate n tunes / figure out the randomness\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
