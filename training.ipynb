{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6625, 0.1789, 0.8318],\n",
      "        [0.9753, 0.3257, 0.7122],\n",
      "        [0.1675, 0.2922, 0.1557],\n",
      "        [0.1326, 0.3882, 0.3282],\n",
      "        [0.0517, 0.7033, 0.9023]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "classifier('We are very angry to show you the ðŸ¤— Transformers library.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter = ['4/4', '6/8', '2/4', '3/4', '9/8', '12/8', '3/2']\n",
    "mode = ['Gmajor',\n",
    " 'Dmajor',\n",
    " 'Amajor',\n",
    " 'Adorian',\n",
    " 'Eminor',\n",
    " 'Edorian',\n",
    " 'Bminor',\n",
    " 'Amixolydian',\n",
    " 'Aminor',\n",
    " 'Dmixolydian',\n",
    " 'Cmajor',\n",
    " 'Fmajor',\n",
    " 'Dminor',\n",
    " 'Gminor',\n",
    " 'Ddorian',\n",
    " 'Gdorian',\n",
    " 'Emajor',\n",
    " 'Gmixolydian',\n",
    " 'Bdorian',\n",
    " 'Cdorian',\n",
    " 'Fdorian',\n",
    " 'Emixolydian',\n",
    " 'Bmixolydian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.add_special_tokens(meter + mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\n",
    "tokenizer.pre_tokenizer = ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(min_frequency=2, special_tokens=meter + mode)\n",
    "tokenizer.train(files=[\"cleaned_tunes.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2/4', 'Ä ', 'Gmajor', 'Ä |:', 'd', '>', 'gfe', '|', 'dBAG', '|', 'E', '/', 'F', '/', 'GE', '/', 'F', '/', 'G', '|', 'BAGF', '|', 'd', '>', 'gfe', '|', 'dBAG', '|', 'E', '/', 'F', '/', 'GE', '/', 'F', '/', 'G', '|', 'BAG', '2', ':||:', 'e', '>', 'fga', '|', 'bagf', '|', 'e', '/', 'f', '/', 'ge', '/', 'f', '/', 'g', '|', 'd', '/', 'c', '/', 'B', '/', 'A', '/', 'BA', '|', 'e', '>', 'fga', '|', 'bagf', '|', 'e', '/', 'f', '/', 'ge', '/', 'f', '/', 'g', '|', 'agg', '2', ':|']\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"2/4 Gmajor |:d>gfe|dBAG|E/F/GE/F/G|BAGF|d>gfe|dBAG|E/F/GE/F/G|BAG2:||:e>fga|bagf|e/f/ge/f/g|d/c/B/A/BA|e>fga|bagf|e/f/ge/f/g|agg2:|\")\n",
    "print(output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer-tune.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tokenizers.Tokenizer' object has no attribute 'alphabet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9370dc55254c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphabet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tokenizers.Tokenizer' object has no attribute 'alphabet'"
     ]
    }
   ],
   "source": [
    "tokenizer.alphabet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
