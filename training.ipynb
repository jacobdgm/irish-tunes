{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers tokenizers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the user's convenience `tokenizers` provides some very high-level classes encapsulating\n",
    "# the overall pipeline for various well-known tokenization algorithm. \n",
    "# Everything described below can be replaced by the ByteLevelBPETokenizer class. \n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Lowercase, NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "\n",
    "# First we create an empty Byte-Pair Encoding model (i.e. not trained model)\n",
    "_tokenizer = Tokenizer(BPE())\n",
    "\n",
    "# Our tokenizer also needs a pre-tokenizer responsible for converting the input to a ByteLevel representation.\n",
    "_tokenizer.pre_tokenizer = ByteLevel()\n",
    "\n",
    "# And finally, let's plug a decoder so we can recover from a tokenized input to the original one\n",
    "_tokenizer.decoder = ByteLevelDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# We initialize our trainer, giving him the details about the vocabulary we want to generate\n",
    "trainer = BpeTrainer(vocab_size=500, show_progress=True, initial_alphabet=ByteLevel.alphabet(), special_tokens=[\n",
    "            \"<s>\",\n",
    "            \"<pad>\",\n",
    "            \"</s>\",\n",
    "            \"<unk>\",\n",
    "            \"<mask>\"\n",
    "        ])\n",
    "\n",
    "\n",
    "_tokenizer.train(files=[\"2021_07_20_tokenized_tunes.txt\"], trainer=trainer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the tokenized data in our specified folder \n",
    "import os\n",
    "save_path = 'tokenized_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "_tokenizer.model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='tokenized_data', vocab_size=403, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d9bc606ff30a3db1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default (download: 9.92 MiB, generated: 40.05 MiB, post-processed: Unknown size, total: 49.98 MiB) to /Users/jckelly/.cache/huggingface/datasets/text/default-d9bc606ff30a3db1/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef7050a60d448e99d85055d24c3e233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /Users/jckelly/.cache/huggingface/datasets/text/default-d9bc606ff30a3db1/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, GenerateMode\n",
    "\n",
    "dataset = load_dataset('text', data_files='2021_07_20_tokenized_tunes.txt', download_mode = GenerateMode.FORCE_REDOWNLOAD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R: polka\\tM: 2/4\\tK: Gmajor\\t|: d > g f e | d B A G | E/ F/ G E/ F/ G | B A G F | d > g f e | d B A G | E/ F/ G E/ F/ G | B A G2 :||: e > f g a | b a g f | e/ f/ g e/ f/ g | d/ c/ B/ A/ B A | e > f g a | b a g f | e/ f/ g e/ f/ g | a g g2 :|\\t'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R',\n",
       " ':',\n",
       " 'Ġpolka',\n",
       " 'ĉ',\n",
       " 'M',\n",
       " ':',\n",
       " 'Ġ2',\n",
       " '/',\n",
       " '4',\n",
       " 'ĉ',\n",
       " 'K',\n",
       " ':',\n",
       " 'ĠGmajor',\n",
       " 'ĉ',\n",
       " '|:',\n",
       " 'Ġd',\n",
       " 'Ġ>',\n",
       " 'Ġg',\n",
       " 'Ġf',\n",
       " 'Ġe',\n",
       " 'Ġ|',\n",
       " 'Ġd',\n",
       " 'ĠB',\n",
       " 'ĠA',\n",
       " 'ĠG',\n",
       " 'Ġ|',\n",
       " 'ĠE',\n",
       " '/',\n",
       " 'ĠF',\n",
       " '/',\n",
       " 'ĠG',\n",
       " 'ĠE',\n",
       " '/',\n",
       " 'ĠF',\n",
       " '/',\n",
       " 'ĠG',\n",
       " 'Ġ|',\n",
       " 'ĠB',\n",
       " 'ĠA',\n",
       " 'ĠG',\n",
       " 'ĠF',\n",
       " 'Ġ|',\n",
       " 'Ġd',\n",
       " 'Ġ>',\n",
       " 'Ġg',\n",
       " 'Ġf',\n",
       " 'Ġe',\n",
       " 'Ġ|',\n",
       " 'Ġd',\n",
       " 'ĠB',\n",
       " 'ĠA',\n",
       " 'ĠG',\n",
       " 'Ġ|',\n",
       " 'ĠE',\n",
       " '/',\n",
       " 'ĠF',\n",
       " '/',\n",
       " 'ĠG',\n",
       " 'ĠE',\n",
       " '/',\n",
       " 'ĠF',\n",
       " '/',\n",
       " 'ĠG',\n",
       " 'Ġ|',\n",
       " 'ĠB',\n",
       " 'ĠA',\n",
       " 'ĠG',\n",
       " '2',\n",
       " 'Ġ:||:',\n",
       " 'Ġe',\n",
       " 'Ġ>',\n",
       " 'Ġf',\n",
       " 'Ġg',\n",
       " 'Ġa',\n",
       " 'Ġ|',\n",
       " 'Ġb',\n",
       " 'Ġa',\n",
       " 'Ġg',\n",
       " 'Ġf',\n",
       " 'Ġ|',\n",
       " 'Ġe',\n",
       " '/',\n",
       " 'Ġf',\n",
       " '/',\n",
       " 'Ġg',\n",
       " 'Ġe',\n",
       " '/',\n",
       " 'Ġf',\n",
       " '/',\n",
       " 'Ġg',\n",
       " 'Ġ|',\n",
       " 'Ġd',\n",
       " '/',\n",
       " 'Ġc',\n",
       " '/',\n",
       " 'ĠB',\n",
       " '/',\n",
       " 'ĠA',\n",
       " '/',\n",
       " 'ĠB',\n",
       " 'ĠA',\n",
       " 'Ġ|',\n",
       " 'Ġe',\n",
       " 'Ġ>',\n",
       " 'Ġf',\n",
       " 'Ġg',\n",
       " 'Ġa',\n",
       " 'Ġ|',\n",
       " 'Ġb',\n",
       " 'Ġa',\n",
       " 'Ġg',\n",
       " 'Ġf',\n",
       " 'Ġ|',\n",
       " 'Ġe',\n",
       " '/',\n",
       " 'Ġf',\n",
       " '/',\n",
       " 'Ġg',\n",
       " 'Ġe',\n",
       " '/',\n",
       " 'Ġf',\n",
       " '/',\n",
       " 'Ġg',\n",
       " 'Ġ|',\n",
       " 'Ġa',\n",
       " 'Ġg',\n",
       " 'Ġg',\n",
       " '2',\n",
       " 'Ġ:|',\n",
       " 'ĉ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(dataset['train']['text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dataset['train'].train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d421c523e74eafab9d68a158a7558a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=' #3', max=7, style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2acc6f7c70b468da023e6ace4810b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=' #2', max=7, style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742649f0b26c48eba54009b10a22a3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=' #1', max=7, style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a79a9782f34991892069db21f2cff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=' #0', max=7, style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200d00f3459d468aa190ed571684bd87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=' #1', max=2, style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f9946feb664dc6ad0b4748ffa5b70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=' #0', max=2, style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d104d3ab0f6b4af2b598f89318c356bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=' #3', max=2, style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397c26acb886484284578c8a36a2c2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=' #2', max=2, style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = datasets.map(lambda x: tokenizer(x['text']), batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, max_length = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I chose the hyperparameters here to get an architecture that\n",
    "# had roughly the same number of paramters as FolkRNN. But I changed a lot of \n",
    "# things and IDK what the trade offs are.\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Config\n",
    "config = GPT2Config(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    n_positions = 512, \n",
    "    n_embd = 384,\n",
    "    n_ctx = 512,\n",
    "    n_layer = 6,\n",
    "    n_inner = 6,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./results',          # output directory\n",
    "  num_train_epochs=10,              # total # of training epochs\n",
    "  per_device_train_batch_size=16,  # batch size per device during training\n",
    "  per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "  evaluation_strategy = \"epoch\",\n",
    "  load_best_model_at_end = True\n",
    "  logging_dir='./logs',            # directory for storing logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=lm_datasets['train'],\n",
    "    eval_dataset=lm_datasets['test'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(start_text = \"a b \"):\n",
    "    # encoding the input text\n",
    "    input_ids = tokenizer.encode(start_text, return_tensors='pt')\n",
    "    # getting out output\n",
    "    beam_output = model.generate(\n",
    "      input_ids,\n",
    "      do_sample=True,\n",
    "      max_length = 128,\n",
    "      top_k=50,\n",
    "      temperature = 0.3,\n",
    "      num_return_sequences=5,\n",
    "      beams=5, \n",
    "    \n",
    "    )\n",
    "    return beam_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate()\n",
    "for i in range(0, 5):\n",
    "    print(tokenizer.decode(output[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO / things to try:\n",
    "#### Pre-processing:\n",
    "    - regenerate dataset with the proper key, mode, and type\n",
    "    - fix tokenization issues\n",
    "    - possibly shift to the same scale \n",
    "#### Tokenization:\n",
    "    - try byte-pair-encoding with key / mode / type\n",
    "    - try getting simple spaced based tokenization to work\n",
    "    - look at other archtitectures for domain specific special tokens\n",
    "    - if we can't get good results, data process + train manually to use our own function \n",
    "#### Data-processing:\n",
    "    - find a better way to create training labels\n",
    "    - tune batch_size and block_size\n",
    "    - figure out how padding works\n",
    "#### Model\n",
    "    - tune architecture parameters\n",
    "    - try alternative architectures (ProphetNet?)\n",
    "#### Training\n",
    "    - tune training parameters\n",
    "#### Generation\n",
    "    - figure out what all of the parameters in generate do \n",
    "    - write our own generation code?\n",
    "#### Evaluation\n",
    "    - look into quality metrics from existing research\n",
    "    - write some code to detect training data plagiarism (generation spitting back an input)\n",
    "    - generate audio from our output for subjective evaluation\n",
    "    - write some code to check for structural integrity ?\n",
    "#### Misc\n",
    "    - figure out a good workflow to train and share model versions \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('checkpoint')\n",
    "model = AutoModelForCausalLM.from_pretrained('checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### retrain the model with \n",
    "### add the metadata\n",
    "\n",
    "## figure out how to best generate n tunes / figure out the randomness\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
