{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust these constants, then run entire notebook.\n",
    "\n",
    "model_name = \"earls_chair\"\n",
    "model_version = \"a\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_path = Path(f\"./models/{model_name}\")\n",
    "checkpoint = model_path / f\"{model_version}/checkpoint/\"\n",
    "metadata_path = Path(f\"./models/metadata.yaml\")\n",
    "\n",
    "output_path = Path(f\"./results/{model_name}/{model_version}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_test_parameters = {\n",
    "    'max_length': 128,\n",
    "    'temperature': 0.8,\n",
    "    'no_repeat_ngram_size': 10,\n",
    "    'do_sample': True,\n",
    "    'top_k': 50,\n",
    "    'early_stopping': True,\n",
    "}\n",
    "\n",
    "test_runs = [\n",
    "    {\n",
    "        'max_length': 128,\n",
    "        'temperature': 0.6,\n",
    "    },\n",
    "    {\n",
    "        'max_length': 128,\n",
    "        'temperature': 0.8,\n",
    "    },\n",
    "    {\n",
    "        'max_length': 128,\n",
    "        'temperature': 0.8,\n",
    "    },\n",
    "    {\n",
    "        'max_length': 128,\n",
    "        'temperature': 1.0,\n",
    "    },\n",
    "    {\n",
    "        'max_length': 256,\n",
    "        'temperature': 0.8,\n",
    "    },\n",
    "    {\n",
    "        'max_length': 512,\n",
    "        'temperature': 0.8,\n",
    "    },\n",
    "    {\n",
    "        'max_length': 128,\n",
    "        'top_k': 50,\n",
    "        'no_repeat_ngram_size': 10,\n",
    "        'do_sample': False,\n",
    "        'num_beams': 5\n",
    "    },\n",
    "]\n",
    "\n",
    "test_parameters = []\n",
    "for i in range(0, len(test_runs)):\n",
    "    run_parameters = base_test_parameters.copy()\n",
    "    run_parameters.update(test_runs[i])\n",
    "    test_parameters.append(run_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_prompts= [\"\"\"R: reel\n",
    "M: 4/4\n",
    "K: Edorian\n",
    "|: E2 B E d E B E |\"\"\",\n",
    "             \n",
    "      \"\"\"R: jig\n",
    "M: 6/8\n",
    "K: Gmajor\n",
    "|: G3 G A B |\"\"\"]\n",
    "\n",
    "\n",
    "test_prompts = [\n",
    "    \n",
    "    # drowsy maggie - Reel, Edor\n",
    "    \"\"\"R: reel\n",
    "M: 4/4\n",
    "K: Edorian\n",
    "|: E2 B E d E B E |\"\"\",\n",
    "    \n",
    "    # kesh jig - Jig, Gmaj\n",
    "    \"\"\"R: jig\n",
    "M: 6/8\n",
    "K: Gmajor\n",
    "|: G3 G A B |\"\"\",\n",
    "    \n",
    "    # john ryan's - Polka, Dmaj \n",
    "    \"\"\"R: polka\n",
    "M: 2/4\n",
    "K: Dmajor\n",
    "d d B/ c/ d/ B/ |\"\"\",\n",
    "    \n",
    "    # king of the fairies - Hornpipe\n",
    "    \"\"\"R: hornpipe\n",
    "M: 4/4\n",
    "K: Edorian\n",
    "|: B,2 | E D E F G F G A\"\"\",\n",
    "    \n",
    "    # inisheer - Waltz\n",
    "    \"\"\"R: waltz\n",
    "M: 3/4\n",
    "K: Gmajor\n",
    "B2 B A B d |\"\"\",\n",
    "    \n",
    "    # the butterfly - Slip Jig\n",
    "    \"\"\"R: slip jig\n",
    "M: 9/8\n",
    "K: Eminor\n",
    "|: B2 E G2 E F3 |\"\"\",\n",
    "    \n",
    "    # banish misfortune - Dmix\n",
    "    \"\"\"R: jig\n",
    "M: 6/8\n",
    "K: Dmixolydian\n",
    "f e d c A G |\"\"\",\n",
    "    \n",
    "    # tam lin - Dmin, low register\n",
    "    \"\"\"R: reel\n",
    "M: 4/4\n",
    "K: Dminor\n",
    "A,2 D A, F A, D A, |\"\"\",\n",
    "    \n",
    "    # cliffs of moher - Ador, high register\n",
    "    \"\"\"R: jig\n",
    "M: 6/8\n",
    "K: Adorian\n",
    "|: a3 b a g |\"\"\",\n",
    "    \n",
    "    # silver spear - no closing barline\n",
    "    \"\"\"R: reel\n",
    "M: 4/4\n",
    "K: Dmajor\n",
    "A |: F A (3 A A A B A F A\"\"\",\n",
    "    \n",
    "    # unspecified tune types and meters\n",
    "    \"R: jig\",\n",
    "    \"R: reel\",\n",
    "    \"R: polka\",\n",
    "    \"R: waltz\",\n",
    "    \"R: hornpipe\",\n",
    "    \"R: slip jig\",\n",
    "]\n",
    "\n",
    "additional = [\n",
    "    \"M: 4/4\",\n",
    "    \"M: 6/8\",\n",
    "    \"M: 3/4\",\n",
    "    \"M: 2/4\",\n",
    "    \"M: 9/8\",\n",
    "\n",
    "    # single notes\n",
    "    \"G\",\n",
    "    \"e\"\n",
    "]\n",
    "\n",
    "if model_name in ['ashplant', 'banish_misfortune']:\n",
    "    test_prompts = test_prompts + additional\n",
    "\n",
    "if model_name in ['earls_chair']:\n",
    "    test_prompts = []\n",
    "    for prompt in additional:\n",
    "        if prompt[0] == 'M':\n",
    "            for mode in ['K:maj', 'K:min', 'K:dor', 'K:mix']:\n",
    "                test_prompts.append('\\n'.join([prompt, mode]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTML Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for compiling html file\n",
    "html_header = \"\"\"<html>\n",
    "<meta charset=\"UTF-8\"/>\n",
    "<script src=\"http://moinejf.free.fr/js/abcweb-1.js\"></script>\n",
    "<script src=\"http://moinejf.free.fr/js/snd-1.js\"></script>\n",
    "<style>svg {display:block}</style>\n",
    "<title></title>\n",
    "</head>\n",
    "<body bgcolor=\"#faf0e6\">\"\"\"\n",
    "\n",
    "html_footer = \"\"\"</body>\n",
    "</html>\"\"\"\n",
    "\n",
    "html_abc_prefix = \"\"\"<script type=\"text/vnd.abc\">\n",
    "%abc-2.2\n",
    "%%pagewidth 14cm\n",
    "%%bgcolor white\n",
    "%%topspace 0\n",
    "%%composerspace 0\n",
    "%%leftmargin 0.8cm\n",
    "%%rightmargin 0.8cm\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "html_abc_postfix = \"\"\"</script>\\n\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, torch, json\n",
    "\n",
    "with open(metadata_path, \"r\") as file:\n",
    "    m = yaml.safe_load(file)\n",
    "    general_metadata = {\n",
    "        'model_name': model_name,\n",
    "        'model_description': m['models'][model_name]['description'],\n",
    "        'dataset': m['models'][model_name]['dataset'],\n",
    "        'version': model_version,\n",
    "        'version_description': m['models'][model_name]['versions'][model_version]['description']\n",
    "    }\n",
    "\n",
    "\n",
    "with open(checkpoint / 'trainer_state.json', \"r\") as file:\n",
    "    m = json.load(file)\n",
    "    training_metadata = {\n",
    "        k: v for k,v in m.items() if k in ['epoch', 'global_step']\n",
    "    }\n",
    "\n",
    "model_metadata_fields = [\n",
    "    \"model_type\",\n",
    "    \"vocab_size\",\n",
    "    \"n_ctx\",\n",
    "    \"n_embd\",\n",
    "    \"n_head\",\n",
    "    \"n_inner\",\n",
    "    \"n_layer\",\n",
    "    \"n_positions\",\n",
    "]\n",
    "\n",
    "model_metadata = {\n",
    "    k: getattr(model.config, k) for k in model_metadata_fields\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(start_text = \"R: reel\", parameters = test_parameters):\n",
    "    # encoding the input text\n",
    "    if model_name in ['cooleys', 'drowsy_maggie', 'earls_chair']:\n",
    "        input_ids = tokenizer.encode(start_text, return_tensors='pt')[:, :-1]\n",
    "    else:\n",
    "        input_ids = tokenizer.encode(start_text, return_tensors='pt')\n",
    "    output = model.generate(input_ids, **parameters)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def test_suite(\n",
    "    path = model_path,\n",
    "    test_parameters = test_parameters,\n",
    "    prompts = test_prompts,\n",
    "):\n",
    "    \n",
    "    current_time = time.strftime(\"%Y-%m-%dT%H:%M:%S\", time.localtime())\n",
    "    output_dir = output_path / current_time\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    txt_file_name = output_dir / \"results.txt\"\n",
    "    html_file_name = output_dir / \"results.html\"\n",
    "    print(f\"running tests on model at {path}\")\n",
    "    print(f\"saving to {txt_file_name} and {html_file_name}\")\n",
    "    \n",
    "    with open(txt_file_name, 'a') as txt_file, open(html_file_name, 'a') as html_file:\n",
    "        \n",
    "        html_file.write(html_header)\n",
    "        \n",
    "        txt_file.write(f\"{current_time}\\n\\n\")\n",
    "        html_file.write(f\"<p>{current_time}</p>\\n\")\n",
    "        \n",
    "        txt_file.write(f\"path_to_model: {path}\\n\\n\")\n",
    "        html_file.write(f\"<p>path_to_model: {path}</p>\\n\")\n",
    "        \n",
    "        # metadata\n",
    "        txt_file.write(f\"general_metadata:\\n\")\n",
    "        html_file.write(f\"<p>general_metadata:\\n<ul>\\n\")\n",
    "        for k in general_metadata:\n",
    "            txt_file.write(f\"    {k}: {general_metadata[k]}\\n\")\n",
    "            html_file.write(f\"<li>{k}: {general_metadata[k]}\\n</li>\\n\")\n",
    "        txt_file.write(\"\\n=========\\n\\n\")\n",
    "        html_file.write(\"</ul>\\n</p>\\n\")\n",
    "        \n",
    "        # model metadata\n",
    "        txt_file.write(f\"model_metadata:\\n\")\n",
    "        html_file.write(f\"<p>model_metadata:\\n<ul>\\n\")\n",
    "        for k in model_metadata:\n",
    "            txt_file.write(f\"    {k}: {model_metadata[k]}\\n\")\n",
    "            html_file.write(f\"<li>{k}: {model_metadata[k]}\\n</li>\\n\")\n",
    "        txt_file.write(\"\\n=========\\n\")\n",
    "        html_file.write(\"</ul>\\n</p>\\n\")\n",
    "        \n",
    "        # training metadata\n",
    "        txt_file.write(f\"training_metadata:\\n\")\n",
    "        html_file.write(f\"<p>training_metadata:\\n<ul>\\n\")\n",
    "        for k in training_metadata:\n",
    "            txt_file.write(f\"    {k}: {training_metadata[k]}\\n\")\n",
    "            html_file.write(f\"<li>{k}: {training_metadata[k]}\\n</li>\\n\")\n",
    "        txt_file.write(\"\\n=========\\n\")\n",
    "        html_file.write(\"</ul>\\n</p>\\n\")\n",
    "        \n",
    "        # prompt index\n",
    "        html_file.write(f\"<p>prompts:\\n<ul>\\n\")\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            html_file.write('<li><a href=\"#prompt{num}\">Prompt {num} - {p}</a></li>\\n'.format(num = i + 1,\n",
    "                                                                                              p = prompt))\n",
    "        html_file.write(\"</ul>\\n</p>\\n\")\n",
    "        \n",
    "        prompt_count = 0\n",
    "        test_count = 0\n",
    "        for prompt in prompts:\n",
    "            if model_name not in ['ashplant']:\n",
    "                prompt = prompt.replace('\\n', '\\t')\n",
    "            prompt_count += 1\n",
    "            print(\"running prompt {}/{}: {}\".format(prompt_count, len(prompts), prompt))\n",
    "            \n",
    "            prompt_counter_text = \"prompt {}:\\n\".format(prompt_count)\n",
    "            prompt_text = \"\\n{}\\n\\n\\n\".format(prompt)\n",
    "            txt_file.write(prompt_counter_text)\n",
    "            txt_file.write(prompt_text)\n",
    "            html_file.write('<h3 id=\"prompt{c}\">Prompt {c} - {p}</h3>\\n'.format(c = prompt_count,\n",
    "                                                                 p = prompt))\n",
    "            html_file.write(\"<p>\")\n",
    "            \n",
    "            \n",
    "            for test in test_parameters:\n",
    "                test_count += 1\n",
    "                output = generate(prompt, test)[0]\n",
    "    \n",
    "                s = tokenizer.decode(output)\n",
    "                if model_name in ['cooleys', 'drowsy_maggie', 'earls_chair']:\n",
    "                    special_tokens=[\n",
    "                        \"<s>\",\n",
    "                        \"</s>\",\n",
    "                        \"<pad>\",\n",
    "                        \"<unk>\",\n",
    "                        \"<mask>\",\n",
    "                    ]\n",
    "                    for t in special_tokens:\n",
    "                        s = s.replace(t, '')\n",
    "                if model_name not in ['ashplant']:\n",
    "                    s = s.replace('\\t', '\\n')\n",
    "                s_html = s.replace('<', '&lt').replace('>', '&gt')\n",
    "                        # training metadata\n",
    "                txt_file.write(f\"test_parameters:\\n\")\n",
    "                html_file.write(f\"<p>test_parameters:\\n<ul>\\n\")\n",
    "                for k in test:\n",
    "                    txt_file.write(f\"    {k}: {test[k]}\\n\")\n",
    "                    html_file.write(f\"<li>{k}: {test[k]}\\n</li>\\n\")\n",
    "                txt_file.write(\"\\n=========\\n\")\n",
    "                html_file.write(\"</ul>\\n</p>\\n\")\n",
    "        \n",
    "                txt_file.write(f\"model_output:\\n\")\n",
    "                html_file.write(f\"model_output:\\n\")\n",
    "                txt_file.write('X: {}\\n'.format(test_count))\n",
    "                txt_file.write(s + \"\\n---\\n\")\n",
    "                html_file.write(\"\\n<pre>X: {}\\n\".format(test_count))\n",
    "                html_file.write(s_html + \"</pre>\\n\\n\")\n",
    "                html_file.write(html_abc_prefix)\n",
    "                html_file.write(\"X: {}\\n\".format(test_count))\n",
    "                html_file.write(s_html.replace(\" \", \"\") + \"\\n\\n\")\n",
    "                html_file.write(html_abc_postfix)\n",
    "                \n",
    "                \n",
    "            \n",
    "            html_file.write(\"</p>\")\n",
    "            txt_file.write(\"\\n---------\\n\\n\")\n",
    "            \n",
    "        \n",
    "        html_file.write(html_footer)\n",
    "        print(\"all prompts completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very sloppy but this is a loop to run the test code for all of the existing models when we want to\n",
    "# TODO: Re-org the code so it's not duplicated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(metadata_path, \"r\") as file:\n",
    "    m = yaml.safe_load(file)\n",
    "\n",
    "model_list = [\n",
    "    (model, version) \n",
    "    for model in m['models'] \n",
    "    for version in m['models'][model]['versions']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, torch\n",
    "\n",
    "for model_name, model_version in model_list:\n",
    "    \n",
    "    model_path = Path(f\"./models/{model_name}\")\n",
    "    checkpoint = model_path / f\"{model_version}/checkpoint/\"\n",
    "    metadata_path = Path(f\"./models/metadata.yaml\")\n",
    "    output_path = Path(f\"./results/{model_name}/{model_version}/\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint, local_files_only=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint, local_files_only=True)\n",
    "\n",
    "    with open(metadata_path, \"r\") as file:\n",
    "        m = yaml.safe_load(file)\n",
    "        general_metadata = {\n",
    "            'model_name': model_name,\n",
    "            'model_description': m['models'][model_name]['description'],\n",
    "            'dataset': m['models'][model_name]['dataset'],\n",
    "            'version': model_version,\n",
    "            'version_description': m['models'][model_name]['versions'][model_version]['description']\n",
    "        }\n",
    "\n",
    "\n",
    "    with open(checkpoint / 'trainer_state.json', \"r\") as file:\n",
    "        m = json.load(file)\n",
    "        training_metadata = {\n",
    "            k: v for k,v in m.items() if k in ['epoch', 'global_step']\n",
    "        }\n",
    "\n",
    "    model_metadata_fields = [\n",
    "        \"model_type\",\n",
    "        \"vocab_size\",\n",
    "        \"n_ctx\",\n",
    "        \"n_embd\",\n",
    "        \"n_head\",\n",
    "        \"n_inner\",\n",
    "        \"n_layer\",\n",
    "        \"n_positions\",\n",
    "    ]\n",
    "\n",
    "    model_metadata = {\n",
    "        k: getattr(model.config, k) for k in model_metadata_fields\n",
    "    }\n",
    "    test_suite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
